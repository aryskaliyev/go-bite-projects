## Chapter 5. Concurrency at Scale
### Error Propagation: Philosophy of Error Propagation
- What errors are? When do they occur, and what benefit do they provide?
	- Errors indicate that your system has entered a state in which it cannot fulfill an operation that a user either explicitly or implicitly requested. Because of this, it needs to relay a few pieces of critical information:
		- **What happened**
			- This is the part of the error that contains information about what happened, e.g., "disk full", "socket closed", or "credentials expired".
		- **When and where it occurred**
			- Errors should always contain a complete stack trace starting with how the call was initiated and ending with where the error was instantiated. The stack trace should *not* be contained in the error message, but should be easily accessible when handling the error up the stack.
			- The error should contain information regarding the context it's running within.
			- The error should contain the time on the machine the error was instantiated on, in UTC.
		- **A friendly user-facing message**
			- The message that gets displayed to the user should be customized to suit your system and its users. It should only contain abbreviated and relevant information from the previous two points. A friendly message is human-centric, gives some indication of whether the issue is transitory, and should be about one line of text.
		- **How the user can get more information**
			- At some point, someone will likely want to know, in detail, what happened when the error occurred. Errors that are presented to users should provide an ID that can be cross-referenced to a corresponding log that displays the full information of the error: time the error occurred (not the time the error was logged), the stack strace -- everything you stuffed into the error when it was created. It can also be helpful to include a hash of the stack trace to aid in aggregating like issues in bug trackers. 

- All errors can be placed in one of two categories:
	- Bugs
	- Known edge cases (e.g., broken network connections, failed disk writes, etc.)

- Bugs are errors that you have not customized to your system, or "raw" errors -- your known edge cases. Raw errors are always bugs. Any error that escapes *our* module without out module's error type can be considered malformed, and a bug. Note that it is only necessary to wrap errors in this fashion at your *own* module boundaries -- public functions/methods -- or when your code can add valuable context.

- Error correctness becomes an emergent property of our system.

- All errors should be logged with as much information as is available. But when displaying errors to users, this is where the distinction between bugs and known edge cases comes in.

- When malformed errors, or bugs, are propagated up to the user, we should also log the error, but then display a friendly message to the user stating something unexpected has happened.

#### Example: Large system with multiple modules: "CLI Component" -> "Intermediary Component" -> "Low Level Component"

- Let's say an error occurs in the "Low Level Component" and we've crafted a well-formed error there to be passed up the stack. *Within the context of the "Low Level Component", this error might be considered well-formed, but within the context of our system, it may not be.* 

```go
func PostReport(id string) error {
	result, err := lowlevel.DoWork()
	if err != nil {
		if _, ok := err.(lowlevel.Error); ok {
			err = Wrap(err, "cannot post report with id %d", id)
		}
		return err
	}
	// ...
}
```

#### Example: Complete example.
```go
type MyError struct {
	Inner error
	Message string
	StackTrace string
	Misc map[string]interface{}
}

func wrapError(err error, messagef string, msgArgs ...interface{}) MyError {
	return MyError{
		Inner: err,
		Message: fmt.Sprintf(messagef, msgArgs...),
		StackTrace: string(debug.Stack()),
		Misc: make(map[string]interface{}),
	}
}

func (err MyError) Error() string {
	return err.Message
}

//--------------------------------------------------------------------------------

// "lowlevel" module

type LowLevelErr struct {
	error
}

func isGloballyExec(path string) (bool, error) {
	info, err := os.Stat(path)
	if err != nil {
		return false, LowLevelErr{(wrapError(err, err.Error()))}
	}
	return info.Mode().Perm()&0100 == 0100, nil
}

// "intermediate" module

type IntermediateErr struct {
	error
}

func runJob(id string) error {
	const jobBinPath = "/bad/job/binary"
	isExecutable, err := isGloballyExec(jobBinPath)
	if err != nil {
		return IntermediateErr{wrapError(
			err,
			"cannot run job %q: requisite binaries not available",
			id,
		)}
	} else if isExecutable == false {
		return wrapError(
			nil,
			"cannot run job %q: requisite binaries are not executable",
			id,
		)
	}
	return exec.Command(jobBinPath, "--id="+id).Run()
}

//--------------------------------------------------------------------------------

func handleError(key int, err error, message string) {
	log.SetPrefix(fmt.Sprintf("[logID: %v]: ", key))
	log.Printf("%#v", err)
	fmt.Printf("[%v] %v", key, message)
}

func main() {
	log.SetOutput(os.Stdout)
	log.SetFlags(log.Ltime|log.LUTC)

	err := runJob("1")
	if err != nil {
		msg := "There was an unexpected issue; please report this as a bug."
		if _, ok := err.(IntermediateErr); ok {
			msg = err.Error()
		}
		handleError(1, err, msg)
	}
}
```

### Timeouts and Cancellation

- Timeouts are crucial to creating a system with behavior you can understand. Cancellation is one natural response to a timeout.

#### What are the reasons we might want our concurrent processes to support timeouts?
- System Saturation
	- If our system is saturated (i.e., if its ability to process requests is at capacity), we may want requests at the edges of our system to time out rather than take a long time to field them. Guidelines for when to time out:
		- If the request is unlikely to repeated when it is timed out.
		- If you don't have the resources to store the requests (e.g., memory for in-memory queues, disk space for persisted queues).
		- if the need for the request, or the data it's sending , will go stale.

- Stale data
	- Sometimes the data has a window within which it must be processed before more relevant data is available, or the need to process the data has expired. If a concurrent process takes longer to process the data than this window, we would want to time out and cancel the concurrent process. For instance, if our concurrent process is dequeing a request after a long wait, the request or its data might have become obsolete during the queuing process. <br> If this window is known beforehand, it would make sense to pass our concurrent process a *context.Context* created with *context.WithDeadline*, or *context.WithTimeout*. If the window is not known beforehand, we'd want the parent of the concurrent process to be able to cancel the concurrent process when the need for the request is no longer present. *context.WithCancel* is perfect for this purpose.

- Attempting to prevent deadlocks
	- It is not unreasonable, and even recommended, to place timeouts on *all* of your concurrent operations to guarantee your system won't deadlock. The timeout period's purpose is only to prevent deadlock, and so it only needs to be short enough that a deadlocked system will unblock in a reasonable amount of time for your use case. <br> Remember that attempting to a void a deadlock by setting a timeout can potentially transform your problem from a system that deadlocks to a system that livelocks. However, it is preferable to chance a livelock and fix that as time permits, than for a deadlock to occur and have a system recoverable only by restart. <br> The goal should be to converge on a system without deadlocks where the timeouts are never triggered.

#### Causes of cancellation, and how to build a concurrent process to handle cancellation gracefully. Reasons why a concurrent process might be canceled:

- Timeouts
	- A timeout is an implicit cancellation.

- User intervention
	- For a good user experience, it's usually advisable to start long-running processes concurrently and then report status back to the user at a polling interval, or allow the users to query for status as they see fit. When there are user-facing concurrent operations, it is therefore also sometimes necessary to allow the users to cancel the operation they've started. 
- Parent cancellation
	- For that matter, if any kind of parent of a concurrent operation -- human or otherwise -- stops, as a child of that parent, we will be canceled.

- Replicated requests
	- We may wish to send data to multiple concurrent processes in an attempt to get a faster response from one of them. When the first one comes back, we would want to cancel the rest of the processes.

#### When a concurrent process is canceled, what does that mean for the algorithm that was executing, and its downstream consumers? When writing concurrent code that can be terminated at any time, what things do you need to take into account?

#### Example: Preemptability of a concurrent process. Assume it's running in its own goroutine
```go
var value interface{}
select {
case <-done:
	return
case value = <-valueStream:
}

result := reallyLongCalculation(value)

select {
case <-done:
	return
case resultStream <- result:
}
``` 
- In the above code, `reallyLongCalculation` is not preemptable. This means that if something attempts to cancel this goroutine while `reallyLongCalculation` is executing, it could be a very long time before we acknowledge the cancellation and halt.

- Let's make `reallyLongCalculation` preemtable.

```go
reallyLongCalculation := func(
	done <-chan interface{},
	value interface{},
) interface{} {
	intermediateResult := longCalculation(value)
	select {
	case <-done:
		return nil
	default:
	}
	return longCalculation(intermediateResult)
}
```

- From the above code, `reallyLongCalculation` is now preemiptable, but we can see that we've only halved the problem: we can only preempt `reallyLongCalculation` in between calls to other; seemingly long-running, function calls. To solve this, we need to make `longCalculation` preemptable as well:

```go
reallyLongCalculation := func(
	done <-chan interface{},
	value interface{},
) interface{} {
	intermediateResult := longCalculation(done, value)
	return longCalculation(done, intermediateResult)
}
```

- If you take this line of reasoning to its logical conclusion, we see that we must do two things: define the period within which our concurrent process is preemptable, and ensure that any functionality that takes more time than this period is itself preemptable. An easy way to do this is to break up the pieces of your goroutine into smaller pieces. You should aim for all *nonpreemptable* atomic operations to complete in less time than the period you've deemed acceptable.

- If our goroutine happens to modify shared state -- e.g., a database, a file, an in-memory data structure -- what happens when the goroutine is canceled? Does your goroutine try and roll back the intermediary work it's done? How long does it have to do this work? Something has told the goroutine that it should halt, so the goroutine shouldn't take too long to roll back its work, right? <br> *If you keep your modifications to any shared state within a tight scope, and/or ensure those modifications are easily rolled back, you can usually handle cancellations pretty well. If possible, build up intermediate results in-memory and then modify state as quickly as possible.*

- There are a few ways to avoid sending duplicate messages. The easiest method is to make it vanishingly unlikely that a parent goroutine will send a cancellation signal after a child goroutine has already reported a result. This requires a bidirectional communication between the stages, i.e., **Heartbeats**. Other approaches are:
	- Accept either the first or last result reported
		- If your algorithm allows it, or your concurrent process is idempotent, you can simply allow for the possibility of duplicate messages in your downstream processes and choose whether to accept the first or last message you receive.
	- Poll the parent goroutine for permission
		- You can use bidirectional communication with your parent to explicitly request permission to send your message.

- When designing concurrent processes, be sure to take into account timeouts and cancellation.

### Heartbeats

- Heartbeats are a way for concurrent processes to signal life to outside parties. They can make testing deterministic.

- There are two different types of heartbeats:
	- Heartbeats that occur on a time interval.
	- Heartbeats that occur at the beginning of a unit of work.

- Heartbeats that occur on a time interval are useful for concurrent code that might be waiting for something else to happen for it to process a unit of work. Because you don't know when that work might come in, your goroutine might be sitting around for a while waiting for something to happen. A heartbeat is a way to signal to its listeners that everything is well, and that silence is expected.

#### Example: Goroutine that exposes a heartbeat

- We must always guard against the fact that no one may be listening to our heartbeat. The results emitted from the goroutine are critical, but the pulses are not.

```go
package main

import (
	"fmt"
	"time"
)

func main() {
	doWork := func(
		done <-chan interface{},
		pulseInterval time.Duration,
	) (<-chan interface{}, <-chan time.Time) {
		heartbeat := make(chan interface{})
		results := make(chan time.Time)
		go func() {
			defer close(heartbeat)
			defer close(results)

			pulse := time.Tick(pulseInterval)
			workGen := time.Tick(2 * pulseInterval)

			sendPulse := func() {
				select {
				case heartbeat <-struct{}{}:
				default:
				}
			}
			sendResult := func(r time.Time) {
				for {
					select {
					case <-done:
						return
					case <-pulse:
						sendPulse()
					case results <- r:
						return
					}
				}
			}

			for {
				select {
				case <-done:
					return
				case <-pulse:
					sendPulse()
				case r := <-workGen:
					sendResult(r)
				}
			}
		}()
		return heartbeat, results
	}

	done := make(chan interface{})
	time.AfterFunc(10 * time.Second, func() { close(done) })

	const timeout = 2 * time.Second
	heartbeat, results := doWork(done, timeout / 2)
	for {
		select {
		case _, ok := <-heartbeat:
			if ok == false {
				return
			}
			fmt.Println("pulse")
		case r, ok := <-results:
			if ok == false {
				return
			}
			fmt.Printf("results %v\n", r.Second())
		case <-time.After(timeout):
			return
		}
	}
}
```

#### The utility for interval-based heartbeats really shines when your goroutine isn't behaving as expected.

#### Example: Incorrectly written goroutine with a panic by stopping the goroutine after only two iterations, and then not closing either of our channels.



















































