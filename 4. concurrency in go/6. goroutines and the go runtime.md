## Chapter 6. Goroutines and the Go Runtime

### Work Stealing

- Go will handle multiplexing goroutines onto OS threads for you. The algorithm it uses to do this is known as a *work stealing* strategy.

- First, let's look at a naive strategy for sharing work across processors, something called *fair scheduling*. In an effort to ensure all processors were equally utilized, we could evenly distribute the load between all available processors. Imagine there are *n* processors and *x* tasks to perform. In the fair scheduling strategy, each processor would get *x/n* tasks.

- Unfortunately, there are problems with this approach. Go models concurrency using a fork-join model. In a fork-join paradigm, tasks are likely dependent on one another, and it turns out naively splitting them among processors will likely cause one of the processors to be underutilized. Not only that, but it can also lead to poor cache locality as tasks that require the same data are scheduled on other processors.

- Let's see whether a FIFO queue can help with basic load-balancing problems: work tasks get scheduled into the queue, and our processors dequeue tasks as they have capacity, or block on joins.<br> It's better than simply dividing the tasks among the processors because it solves the problem with underutilized processors, but we've now introduced a centralized data structure that all the processors must use. The problem is that continually entering and exiting critical sections is extremely costly. Not only that, but our cache locality problems have only been exacerbated: we're now going to load the centralized queue into each processor's cache every time it wants to enqueue or dequeue a task. Still, for coarse-grained operations, this can be a valid approach. However, goroutines usually aren't coarse-grained, so a centralized queue probably isn't a great choice for our work scheduling algorithm.

- The next leap we could make is to decentralize the work queues. We could give each processor its own thread and a double-ended queue, or *deque*. OK, we've solved our problem with a central data structure under high contention, but what about the problems with cache locality and processor utilization?
